{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **In this short assignment, you will be asked to build a neural network to classify different categories of clothes using the Fashion-MNIST dataset.**\n",
        "\n",
        "Please submit your code and answers to the questions in the form of a Jupyter notebook, containing Pytorch code with explanations, along with a Markdown text explaining different parts if needed."
      ],
      "metadata": {
        "id": "Y0R5TMxvF3n_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1wGGWZVDY5x"
      },
      "source": [
        "# Short Assignment: Classifying Fashion-MNIST\n",
        "\n",
        "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
        "\n",
        "<img src='https://github.com/RankJay/Deep-Learning-with-Pytorch-from-Facebook-Udacity/blob/master/Introduction%20to%20PyTorch/assets/fashion-mnist-sprite.png?raw=true' width=500px>\n",
        "\n",
        "In this notebook, you'll build your own neural network. For the most part, the code would be very similar to what we had for the MNIST dataset.\n",
        "\n",
        "First off, let's load the dataset through torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWX9wGh6DY5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04346527-28d7-4650-e2c9-9319dfe9c376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:00<00:00, 114MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 3.60MB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 61.8MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qHjKJKlDY50"
      },
      "source": [
        "Here we can see one of the images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following function to show the images. There is no need to understand the code for this function in detail."
      ],
      "metadata": {
        "id": "HFIP5tIJmh5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "axTgpzsinH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "axHoiH9imdQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4csbTkI9DY50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "6537993b-5dbd-40eb-ddb8-2b62630b2406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7, 3, 9, 1, 1, 1, 9, 7, 4, 7, 2, 9, 9, 4, 3, 7, 8, 6, 7, 0, 3, 5, 4, 9,\n",
            "        2, 6, 0, 4, 5, 0, 7, 1, 2, 0, 8, 6, 3, 8, 2, 9, 2, 4, 2, 2, 9, 2, 3, 9,\n",
            "        6, 3, 9, 2, 5, 5, 0, 8, 0, 3, 1, 9, 7, 8, 2, 2])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image, label = next(iter(trainloader))\n",
        "imshow(image[0,:]);\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ZBj_ZmDY50"
      },
      "source": [
        "## Building the network\n",
        "\n",
        "**Exercise 1**: Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the log-softmax from the forward pass. We also suggest you use 3 hidden layers of sizes 256, 128, 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_itXp_2pDY50"
      },
      "outputs": [],
      "source": [
        "# TODO: Define your network architecture here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au64xX2IDY50"
      },
      "source": [
        "## Train the network\n",
        "\n",
        "Now you can train your network. First we define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
        "\n",
        "Then write the training code. The training pass is a fairly straightforward process:\n",
        "\n",
        "* Make a forward pass through the network to get the logits\n",
        "* Use the logits to calculate the loss\n",
        "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
        "* Take a step with the optimizer to update the weights\n",
        "\n",
        "No worries if you don't get everything now. We will talk about training neural networks in detail in Class 3. To finish the following exercise, you don't need to understand the details of training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2**: Fill in the missing code to flatten each image in a batch into a 784 long vector."
      ],
      "metadata": {
        "id": "E5UQOI7DHLjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few options here: [`images.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`images.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`images.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "* `images.reshape(a, b)` will return a new tensor with the same data as `images` with `a` rows and `b` columns.\n",
        "* `images.resize_(a, b)` returns the same tensor with shape `(a,b)`. Here note that the underscore at the end of the method denotes that this method is performed **in-place** operation that changes directly the content of a given Tensor without making a copy. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
        "* `images.view(a, b)` will return a new tensor with the same data as `images` with `a` rows and `b` columns.\n",
        "\n",
        "I usually use `.view()`, but any of the three methods will work for this."
      ],
      "metadata": {
        "id": "BTbjV99VmuwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKqlM7ktDY51"
      },
      "outputs": [],
      "source": [
        "# Train the network here\n",
        "from torch import optim\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003) # here we are using Adam optimizers\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "\n",
        "        # TODO: Flatten MNIST images into a 784 long vector\n",
        "\n",
        "        #Training pass\n",
        "        output=model(images)\n",
        "        loss = criterion(output,labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test out your network"
      ],
      "metadata": {
        "id": "mbwjGPZSl02h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3**: Fill in the missing code to calculate the class probabilities `ps`."
      ],
      "metadata": {
        "id": "2QeD_0TzHcYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following function to view the classification results. There is no need to understand the code for this function in detail."
      ],
      "metadata": {
        "id": "x08pQ7Nxm7o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "veyMbtJ_m7UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSZjM058DY51"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# Test out your network!\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "img = images[0]\n",
        "# Convert 2D image to 1D vector\n",
        "img = img.resize_(1, 784)\n",
        "\n",
        "# Calculate the ouput (softmax) for img\n",
        "with torch.no_grad():\n",
        "    logps = model(img)\n",
        "\n",
        "# TODO: Calculate the class probabilities for img\n",
        "ps =\n",
        "\n",
        "# Plot the image and probabilities\n",
        "view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}